from pathlib import Path
from typing import List, Dict
import os
import subprocess
import json
from src.utils.logger import get_logger

logger = get_logger(__name__)


class RepositoryAnalyzer:
    """ãƒ­ãƒ¼ã‚«ãƒ«ãƒªãƒã‚¸ãƒˆãƒªã®åˆ†æ"""

    IGNORE_DIRS = {
        ".git",
        ".venv",
        "node_modules",
        "__pycache__",
        "dist",
        "build",
        ".pytest_cache",
        ".mypy_cache",
        "venv",
        "env",
    }
    CODE_EXTENSIONS = {
        ".py",
        ".js",
        ".ts",
        ".tsx",
        ".jsx",
        ".java",
        ".go",
        ".rs",
        ".c",
        ".cpp",
        ".h",
        ".hpp",
        ".rb",
        ".php",
        ".swift",
        ".kt",
    }

    def __init__(self, repo_path: Path):
        """
        Args:
            repo_path: åˆ†æå¯¾è±¡ã®ãƒªãƒã‚¸ãƒˆãƒªãƒ‘ã‚¹
        """
        self.repo_path = repo_path
        self._code_parser = None

    def get_file_tree(self, max_depth: int = 3) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ„ãƒªãƒ¼ã‚’å–å¾—ï¼ˆMarkdownå½¢å¼ï¼‰

        Args:
            max_depth: æœ€å¤§æ¢ç´¢æ·±åº¦

        Returns:
            Markdownå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ„ãƒªãƒ¼
        """
        tree_lines = []

        def walk_dir(dir_path: Path, depth: int = 0):
            if depth > max_depth:
                return

            try:
                entries = sorted(
                    dir_path.iterdir(), key=lambda x: (not x.is_dir(), x.name)
                )
                for entry in entries:
                    if entry.name in self.IGNORE_DIRS:
                        continue

                    indent = "  " * depth
                    if entry.is_dir():
                        tree_lines.append(f"{indent}- {entry.name}/")
                        walk_dir(entry, depth + 1)
                    else:
                        tree_lines.append(f"{indent}- {entry.name}")
            except PermissionError:
                pass

        walk_dir(self.repo_path)
        return "\n".join(tree_lines)

    def search_files(self, pattern: str) -> List[Path]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢

        Args:
            pattern: Globãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: "**/*.py"ï¼‰

        Returns:
            ãƒãƒƒãƒã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        return list(self.repo_path.glob(pattern))

    def read_code_files(self, file_paths: List[Path], max_chars: int = 50000) -> str:
        """è¤‡æ•°ã®ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€é€£çµ

        Args:
            file_paths: èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            max_chars: æœ€å¤§æ–‡å­—æ•°

        Returns:
            é€£çµã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
        """
        content_parts = []
        total_chars = 0

        for file_path in file_paths:
            if file_path.suffix not in self.CODE_EXTENSIONS:
                continue

            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                if total_chars + len(content) > max_chars:
                    remaining = max_chars - total_chars
                    content = content[:remaining] + "\n... (truncated)"

                relative_path = file_path.relative_to(self.repo_path)
                content_parts.append(
                    f"## File: {relative_path}\n```{file_path.suffix[1:]}\n{content}\n```\n"
                )
                total_chars += len(content)

                if total_chars >= max_chars:
                    break

            except Exception:
                continue

        return "\n".join(content_parts)

    def get_project_summary(self) -> Dict[str, any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’å–å¾—

        Returns:
            ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã€è¡Œæ•°ã€ä¸»è¦è¨€èªã‚’å«ã‚€ã‚µãƒãƒªãƒ¼
        """
        file_counts = {}
        total_lines = 0

        for root, dirs, files in os.walk(self.repo_path):
            dirs[:] = [d for d in dirs if d not in self.IGNORE_DIRS]

            for file in files:
                ext = Path(file).suffix
                if ext in self.CODE_EXTENSIONS:
                    file_counts[ext] = file_counts.get(ext, 0) + 1

                    try:
                        file_path = Path(root) / file
                        with open(file_path, "r", encoding="utf-8") as f:
                            total_lines += sum(1 for _ in f)
                    except Exception:
                        pass

        primary_language = (
            max(file_counts.items(), key=lambda x: x[1])[0] if file_counts else None
        )

        return {
            "file_counts": file_counts,
            "total_lines": total_lines,
            "primary_language": primary_language,
        }

    @property
    def code_parser(self):
        """CodeParserã®lazy loading"""
        if self._code_parser is None:
            from src.repository.code_parser import CodeParser

            self._code_parser = CodeParser()
        return self._code_parser

    def ripgrep_search(self, keywords: List[str]) -> List[Path]:
        """ripgrepã‚’ä½¿ã£ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢

        Args:
            keywords: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ

        Returns:
            ãƒãƒƒãƒã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        if not keywords:
            return []

        logger.info(f"ğŸ” [File Search] Starting ripgrep keyword search: {keywords}")

        matched_files = set()

        for keyword in keywords:
            logger.info(f"   ğŸ” Searching: '{keyword}'...")
            try:
                # ripgrepã‚’JSONå‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
                result = subprocess.run(
                    [
                        "rg",
                        "--json",
                        "--iglob",
                        "*.py",  # Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
                        "--iglob",
                        "!.venv",  # .venvã‚’é™¤å¤–
                        "--iglob",
                        "!__pycache__",
                        keyword,
                        str(self.repo_path),
                    ],
                    capture_output=True,
                    text=True,
                    timeout=5,
                )

                # JSONå‡ºåŠ›ã‚’è§£æ
                for line in result.stdout.splitlines():
                    try:
                        data = json.loads(line)
                        if data.get("type") == "match":
                            file_path = Path(data["data"]["path"]["text"])
                            matched_files.add(file_path)
                    except json.JSONDecodeError:
                        continue

            except (subprocess.TimeoutExpired, FileNotFoundError) as e:
                # ripgrep not found or timeout
                logger.warning(f"   âš ï¸ ripgrep error, falling back to glob: {e}")
                # Fallback: glob search
                matched_files.update(self.search_files(f"**/*{keyword}*"))

        logger.info(f"âœ… [Search Complete] Found {len(matched_files)} files")
        for i, file_path in enumerate(list(matched_files)[:10], 1):
            logger.info(f"   {i}. {file_path}")
        if len(matched_files) > 10:
            logger.info(f"   ... and {len(matched_files) - 10} more files")

        return list(matched_files)

    def read_code_intelligently(
        self, keywords: List[str], max_functions: int = 20, max_chars: int = 50000
    ) -> str:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦é–¢é€£ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’è³¢ãæŠ½å‡º

        Args:
            keywords: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
            max_functions: æœ€å¤§é–¢æ•°/ã‚¯ãƒ©ã‚¹æ•°
            max_chars: æœ€å¤§æ–‡å­—æ•°

        Returns:
            Markdownå½¢å¼ã®é–¢é€£ã‚³ãƒ¼ãƒ‰
        """
        logger.info(f"ğŸ§  [Smart Code Extraction] Starting (max {max_functions} functions, {max_chars} chars)")

        # Search files with ripgrep
        relevant_files = self.ripgrep_search(keywords)

        if not relevant_files:
            # Fallback: keyword-based glob search
            relevant_files = []
            for keyword in keywords:
                relevant_files.extend(self.search_files(f"**/*{keyword}*"))
            relevant_files = list(set(relevant_files))[:10]

        content_parts = []
        total_chars = 0
        function_count = 0

        logger.info(f"ğŸŒ² [tree-sitter Parse] Parsing {len(relevant_files)} files...")

        for file_path in relevant_files:
            if file_path.suffix != ".py":
                continue

            logger.info(f"   ğŸ“„ Parsing: {file_path}")

            # Extract functions/classes with tree-sitter
            definitions = self.code_parser.extract_relevant_code(file_path, keywords)

            if definitions:
                logger.info(f"      âœ“ Extracted {len(definitions)} functions/classes")

            if not definitions:
                continue

            relative_path = file_path.relative_to(self.repo_path)

            for definition in definitions:
                if function_count >= max_functions:
                    break

                code = definition["code"]
                if total_chars + len(code) > max_chars:
                    break

                # Markdownå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                def_type = definition["type"]
                def_name = definition["name"]
                docstring = definition["docstring"]

                header = f"## File: {relative_path} - {def_type.capitalize()}: {def_name}"
                if docstring:
                    header += f"\n**Description**: {docstring[:200]}..."

                content_parts.append(f"{header}\n```python\n{code}\n```\n")

                total_chars += len(code)
                function_count += 1

            if function_count >= max_functions or total_chars >= max_chars:
                break

        if not content_parts:
            logger.warning("âš ï¸ [Extraction Complete] No relevant code found")
            return "No relevant code found."

        logger.info(f"âœ… [Extraction Complete] Extracted {function_count} functions/classes")
        logger.info(f"   ğŸ“Š Total characters: {total_chars} characters")
        logger.info(f"   ğŸ’° Estimated tokens: ~{total_chars // 4} tokens")

        return "\n".join(content_parts)
