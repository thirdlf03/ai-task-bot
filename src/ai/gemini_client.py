from google import genai
from typing import List, Dict, Any
from pydantic import ValidationError
from src.config import settings
from src.utils.logger import get_logger
from src.ai.schemas import AnalysisResponse, SubtaskResponse, KeywordResponse

logger = get_logger(__name__)


class GeminiClient:
    """Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self):
        self.client = genai.Client(api_key=settings.GEMINI_API_KEY)
        self.model_name = "gemini-3-flash-preview"

    async def analyze_code(
        self, code_context: str, task_description: str
    ) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¿ã‚¹ã‚¯èª¬æ˜ã‹ã‚‰å®Ÿè£…çŠ¶æ³ã‚’åˆ†æ

        Args:
            code_context: ãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            task_description: ã‚¿ã‚¹ã‚¯ã®èª¬æ˜

        Returns:
            Dict containing is_implemented, confidence, reasoning, related_files, missing_components
        """

        prompt = f"""
ã‚ãªãŸã¯ã‚³ãƒ¼ãƒ‰åˆ†æã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ãŒãƒªãƒã‚¸ãƒˆãƒªã«å®Ÿè£…æ¸ˆã¿ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¹ã‚¯å†…å®¹
{task_description}

## ã‚³ãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{code_context}
"""

        logger.info("Analyzing code implementation status with Gemini...")

        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": AnalysisResponse.model_json_schema(),
                }
            )

            # Pydanticã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            result = AnalysisResponse.model_validate_json(response.text)
            result_dict = result.model_dump()

            logger.info(
                f"Analysis complete: is_implemented={result_dict['is_implemented']}, confidence={result_dict['confidence']}"
            )
            return result_dict

        except ValidationError as e:
            logger.error(f"âŒ [Pydantic Validation Failed] {e}")
            logger.error(f"Response text: {response.text}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™ï¼ˆåˆ†æå¤±æ•— = å®Ÿè£…ã•ã‚Œã¦ã„ãªã„ã¨åˆ¤å®šï¼‰
            return {
                "is_implemented": False,
                "confidence": 0.0,
                "reasoning": f"Parse failed: {str(e)}",
                "related_files": [],
                "missing_components": [],
            }
        except Exception as e:
            logger.error(f"âŒ [Analysis Failed] {e}")
            raise

    async def break_down_task(
        self, task_description: str, repo_context: str
    ) -> List[Dict[str, Any]]:
        """ã‚¿ã‚¹ã‚¯ã‚’1PRç²’åº¦ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£

        Args:
            task_description: ã‚¿ã‚¹ã‚¯ã®èª¬æ˜
            repo_context: ãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            List of subtasks with title, description, estimated_effort, dependencies, acceptance_criteria
        """

        prompt = f"""
ã‚ãªãŸã¯ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’1PRï¼ˆPull Requestï¼‰ç²’åº¦ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¹ã‚¯å†…å®¹
{task_description}

## ãƒªãƒã‚¸ãƒˆãƒªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{repo_context}

å„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã¯ä»¥ä¸‹ã®æ¡ä»¶ã‚’æº€ãŸã™å¿…è¦ãŒã‚ã‚Šã¾ã™:
- 1ã¤ã®PRã§å®Œçµã§ãã‚‹ç²’åº¦
- ç‹¬ç«‹ã—ã¦å®Ÿè£…ãƒ»ãƒ†ã‚¹ãƒˆå¯èƒ½
- æ˜ç¢ºãªå®Œäº†æ¡ä»¶ãŒã‚ã‚‹
- æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ã€å‚è€ƒã‚³ãƒ¼ãƒ‰ã¨ã—ã¦æŠœç²‹ã‚’å«ã‚ã‚‹
- **ã‚¿ã‚¤ãƒˆãƒ«ã¯Conventional Commitså½¢å¼ã«å¾“ã†**: type(scope): description
  - type: feat, fix, docs, style, refactor, perf, test, chore ã®ã„ãšã‚Œã‹
  - scope: å¤‰æ›´ã®ç¯„å›²ï¼ˆä¾‹: api, ui, dbï¼‰- ã‚ªãƒ—ã‚·ãƒ§ãƒ³
  - description: **æ—¥æœ¬èª**ã§ç°¡æ½”ãªèª¬æ˜ã‚’è¨˜è¿°ï¼ˆå°æ–‡å­—ã§å§‹ã¾ã‚‹ï¼‰
  - ä¾‹: "feat(reminder): ãƒªãƒã‚¤ãƒ³ãƒ€ãƒ¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ", "fix(db): æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®å•é¡Œã‚’ä¿®æ­£"

æ³¨æ„: å‚è€ƒã‚³ãƒ¼ãƒ‰ãŒãªã„å ´åˆã€reference_codeã¯nullã«ã—ã¦ãã ã•ã„ã€‚
"""

        logger.info("ğŸ¤– [AI Processing] Starting task breakdown...")
        logger.info(f"ğŸ“Š Repository context: {len(repo_context)} characters")

        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": SubtaskResponse.model_json_schema(),
                }
            )

            logger.info(f"ğŸ’­ [Gemini Response Length] {len(response.text)} characters")
            logger.info(f"ğŸ’­ [Gemini Response Preview]\n{response.text[:1000]}...")

            # Pydanticã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            result = SubtaskResponse.model_validate_json(response.text)
            subtasks = [subtask.model_dump() for subtask in result.subtasks]

            logger.info(f"âœ… [Task Breakdown Complete] Created {len(subtasks)} subtasks")

            # Log details of each subtask
            for i, subtask in enumerate(subtasks, 1):
                logger.info(f"ğŸ“Œ Subtask {i}/{len(subtasks)}: {subtask.get('title', 'No title')}")
                logger.info(f"   â”œâ”€ Size: {subtask.get('estimated_effort', 'Unknown')}")
                logger.info(f"   â”œâ”€ Dependencies: {subtask.get('dependencies', [])}")
                logger.info(f"   â””â”€ Reference code: {'Yes' if subtask.get('reference_code') else 'No'}")

            return subtasks

        except ValidationError as e:
            logger.error(f"âŒ [Pydantic Validation Failed] {e}")
            logger.error(f"Response text: {response.text}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç©ºãƒªã‚¹ãƒˆã§ã¯ãªãã‚¨ãƒ©ãƒ¼ã‚’æŠ•ã’ã‚‹
            raise ValueError(f"Failed to parse task breakdown response: {e}") from e
        except Exception as e:
            logger.error(f"âŒ [Task Breakdown Failed] {e}")
            raise

    async def extract_keywords(self, task_description: str) -> List[str]:
        """ã‚¿ã‚¹ã‚¯èª¬æ˜ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º

        Args:
            task_description: ã‚¿ã‚¹ã‚¯ã®èª¬æ˜

        Returns:
            æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        """
        prompt = f"""
ã‚ãªãŸã¯ã‚³ãƒ¼ãƒ‰åˆ†æã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯å†…å®¹ã‹ã‚‰ã€é–¢é€£ã™ã‚‹ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¹ã‚¯å†…å®¹
{task_description}

æ³¨æ„:
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯3-5å€‹ç¨‹åº¦
- ãƒ•ã‚¡ã‚¤ãƒ«åã‚„ãƒ•ã‚©ãƒ«ãƒ€åã«å«ã¾ã‚Œãã†ãªå˜èªã‚’é¸ã¶
- ä¾‹: "èªè¨¼æ©Ÿèƒ½ã‚’è¿½åŠ " â†’ ["auth", "login", "user"]
"""

        logger.info("ğŸ¤– [AI Processing] Starting keyword extraction...")
        logger.info(f"ğŸ“ Task description: {task_description}")

        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": KeywordResponse.model_json_schema(),
                }
            )

            logger.info(f"ğŸ’­ [Gemini Response]\n{response.text[:500]}...")

            # Pydanticã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            result = KeywordResponse.model_validate_json(response.text)
            keywords = result.keywords

            logger.info(f"ğŸ”‘ [Extraction Complete] Keywords: {keywords}")
            logger.info(f"ğŸ’¡ [AI Decision] Searching files with these keywords")

            return keywords

        except ValidationError as e:
            logger.error(f"âŒ [Pydantic Validation Failed] {e}")
            logger.error(f"Response text: {response.text}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            logger.warning("âš ï¸ Keyword extraction failed, returning empty list")
            return []
        except Exception as e:
            logger.error(f"âŒ [Keyword Extraction Failed] {e}")
            raise
